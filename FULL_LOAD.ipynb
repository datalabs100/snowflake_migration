{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "g3ynab5lvkihgiussrxj",
   "authorId": "7839801670957",
   "authorName": "MZS1988",
   "authorEmail": "datalabs300@gmail.com",
   "sessionId": "57562ae2-3536-4881-889b-688589a830cc",
   "lastEditTime": 1758935575272
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "from datetime import datetime\n#from datetime import datetime, timedelta\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import sql_expr, lit\nimport sys\n\n# ---------- Configuration ----------\nfolder_path = datetime.utcnow().strftime(\"%Y/%b/%d/\").title()  # e.g. 2025/Sep/03/\nprint(\"üìå Using folder path:\", folder_path)\n\n#tomorrow = datetime.utcnow() + timedelta(days=-1)\n#folder_path = tomorrow.strftime(\"%Y/%b/%d/\").title()\n#print(\"üìå Using folder path:\", folder_path)\n\n\n\nconnection_parameters = {\n    \"account\": \"MJ13681.ap-southeast-1\",\n    \"user\": \"mzs1988\",\n    \"password\": \"Datalabs@193001\",\n    \"role\": \"ACCOUNTADMIN\",\n    \"warehouse\": \"COMPUTE_WH\",\n    \"database\": \"DEV\",\n    \"schema\": \"DATASCIENCE\"\n}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "source": "# Entities and patterns\nentities = {\n    \"customers\": {\n        \"bronze_table\": \"bronze_customers\",\n        \"silver_table\": \"silver_dlt_customers\",\n        \"pattern\": r\".*Customers_.*?/part-.*\\.parquet\",\n        \"required_cols\": [\"customer_id\", \"customer_name\"],\n        \"casts\": {\n            \"customer_id\": \"int\",\n            \"customer_name\": \"string\",\n            \"start_date\": \"timestamp\"\n        }\n    },\n    \"products\": {\n        \"bronze_table\": \"bronze_products\",\n        \"silver_table\": \"silver_dlt_products\",\n        \"pattern\": r\".*Products_.*?/part-.*\\.parquet\",\n        \"required_cols\": [\"product_id\", \"product_name\"],\n        \"casts\": {\n            \"product_id\": \"string\",\n            \"product_name\": \"string\",\n            \"start_date\": \"timestamp\"\n        }\n    },\n    \"orders\": {\n        \"bronze_table\": \"bronze_orders\",\n        \"silver_table\": \"silver_dlt_orders\",\n        \"pattern\": r\".*Orders_.*?/part-.*\\.parquet\",\n        \"required_cols\": [\"order_id\", \"customer_id\", \"product_id\"],\n        \"casts\": {\n            \"order_id\": \"string\",\n            \"customer_id\": \"int\",\n            \"product_id\": \"string\",\n            \"start_date\": \"timestamp\"\n        }\n    }\n}\n\n# ---------- Create Snowflake session ----------\nsession = Session.builder.configs(connection_parameters).create()\nprint(\"‚úÖ Snowflake session created\")\n\n# helper SQL expressions\n#now_expr = \"TO_TIMESTAMP_NTZ(TO_CHAR(CURRENT_TIMESTAMP(), 'YYYY-MM-DD HH24:MI:SS'))\"\nnow_expr = \"TO_TIMESTAMP_NTZ(TO_CHAR(CONVERT_TIMEZONE('UTC', CURRENT_TIMESTAMP()), 'YYYY-MM-DD HH24:MI:SS'))\"\nopen_end_date_literal = \"TO_TIMESTAMP_NTZ('2099-12-31 23:59:59')\"\n\ndef stage_has_files(entity_name):\n    \"\"\"\n    Use LIST @stage/path to check if any object exists for today's folder for the entity.\n    Returns True if at least one object found.\n    \"\"\"\n    stage_path = f\"@bronze_csv/{entity_name}/Full_load/csv_files/{folder_path}\"\n    try:\n        df = session.sql(f\"LIST {stage_path};\")\n        rows = df.collect()\n        return len(rows) > 0\n    except Exception as e:\n        # If LIST fails (e.g., path doesn't exist) treat as no files\n        print(f\"‚ö†Ô∏è LIST failed for {stage_path}: {str(e)}\")\n        return False",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "# ---------- Step 1: Overwrite bronze tables if today's parquet present ----------\nfor entity, meta in entities.items():\n    bronze_table = meta[\"bronze_table\"]\n    stage_subpath = f\"@bronze_csv/{entity}/Full_load/csv_files/{folder_path}\"\n    pattern = meta[\"pattern\"]\n\n    has_files = stage_has_files(entity)\n    if has_files:\n        print(f\"üì• Found parquet files for {entity} at {stage_subpath} ‚Äî will overwrite {bronze_table}.\")\n        # Truncate and load from stage into bronze table\n        # TRUNCATE then COPY INTO table\n        try:\n            session.sql(f\"TRUNCATE TABLE {bronze_table};\").collect()\n            copy_sql = f\"\"\"\n            COPY INTO {bronze_table}\n            FROM {stage_subpath}\n            FILE_FORMAT = (FORMAT_NAME = my_parquet_format)\n            MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE\n            PATTERN = '{pattern}';\n            \"\"\"\n            \n            session.sql(copy_sql).collect()\n            print(f\"‚úÖ Overwrote table {bronze_table} from stage {stage_subpath}\")\n        except Exception as e:\n            print(f\"‚ùå Error loading {entity} into {bronze_table}: {e}\")\n            session.close()\n            raise\n    else:\n        print(f\"‚ÑπÔ∏è No parquet files found for {entity} at {stage_subpath}. Keeping existing {bronze_table} data.\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4dd2ce13-a964-451c-a02a-50b4a32a1e59",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "df = session.table(\"bronze_products\")\ndf.show(15)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d95a9570-0090-49b3-b89d-e583feed6866",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# ---------- Step 2: Cleaning bronze (remove nulls, dedupe, cast, add load_date/end_date) ----------\n# We'll create temporary tables bronze_{entity}_clean to use in merge logic.\nfor entity, meta in entities.items():\n    bronze_table = meta[\"bronze_table\"]\n    clean_table = f\"{bronze_table}_clean\"\n    required_cols = meta[\"required_cols\"]\n    casts = meta[\"casts\"]\n\n    # Build WHERE clause for non-null required cols\n    not_null_conditions = \" AND \".join([f\"{col} IS NOT NULL\" for col in required_cols])\n\n    # Build CAST expressions (use SELECT ... )\n    select_expressions = []\n    for col_name, dtype in casts.items():\n        if dtype.lower() == \"int\":\n            select_expressions.append(f\"TRY_TO_NUMBER({col_name}) AS {col_name}\")\n        elif dtype.lower() == \"timestamp\":\n            # Ensure start_date not in the future\n            select_expressions.append(f\"{col_name} AS {col_name}\")\n        else:\n            # string ‚Üí trim\n            select_expressions.append(f\"TRIM({col_name}) AS {col_name}\")\n\n    select_clause = \",\\n        \".join(select_expressions)\n\n    # Create cleaned temporary table\n    create_clean_sql = f\"\"\"\n    CREATE OR REPLACE TEMPORARY TABLE {clean_table} AS\n    SELECT DISTINCT\n        {select_clause},\n        {now_expr} AS load_date,\n        {open_end_date_literal} AS end_date\n    FROM {bronze_table}\n    WHERE {not_null_conditions}\n      AND start_date IS NOT NULL\n      AND start_date <= {now_expr}\n    ;\n    \"\"\"\n    session.sql(create_clean_sql).collect()\n    print(f\"‚úÖ Created cleaned table: {clean_table}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7810c86a-0541-449b-8e05-64e45112f296",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "df = session.table(\"bronze_products_clean\").order_by(\"product_id\")\ndf.show(15)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1a581fd-36e6-4c22-b694-518919087556",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "# ---------- Step 3: Merge cleaned bronze into silver_dlt tables (SCD Type 2) ----------\nfor entity, meta in entities.items():\n    bronze_clean = f\"{meta['bronze_table']}_clean\"\n    silver_table = meta['silver_table']\n\n    # Determine key and columns to compare\n    if entity == \"customers\":\n        key = \"customer_id\"\n        compare_cols = [\"customer_name\", \"start_date\"]\n    elif entity == \"products\":\n        key = \"product_id\"\n        compare_cols = [\"product_name\", \"start_date\"]\n    else:  # orders\n        key = \"order_id\"\n        compare_cols = [\"customer_id\", \"product_id\", \"start_date\"]\n\n    # Use IS DISTINCT FROM to safely detect differences (handles NULLs)\n    change_conditions = \" OR \".join([f\"(tgt.{c} IS DISTINCT FROM src.{c})\" for c in compare_cols])\n\n    # -----------------------\n    # 1) EXPIRE old active rows for records that have changed\n    #    Set old row's end_date = src.start_date (truncate to seconds)\n    # -----------------------\n    expire_changed_sql = f\"\"\"\n    UPDATE {silver_table} tgt\n    SET end_date = DATE_TRUNC('SECOND', src.start_date)\n    FROM {bronze_clean} src\n    WHERE tgt.{key} = src.{key}\n      AND tgt.end_date = {open_end_date_literal}\n      AND ({change_conditions});\n    \"\"\"\n    session.sql(expire_changed_sql).collect()\n    print(f\"‚úÖ Expired old active rows in {silver_table} for changed records (end_date set to new start_date)\")\n\n    # -----------------------\n    # 2) INSERT new rows:\n    #    Insert any src rows that do not currently have an active target row.\n    #    This inserts brand-new keys AND also inserts new rows for updated keys\n    #    because the previous step expired the active row for updated keys.\n    # -----------------------\n    insert_new_sql = f\"\"\"\n    INSERT INTO {silver_table} ({', '.join([key] + compare_cols + ['load_date', 'end_date'])})\n    SELECT\n      src.{key},\n      {', '.join(['src.' + c for c in compare_cols])},\n      DATE_TRUNC('SECOND', CONVERT_TIMEZONE('UTC', CURRENT_TIMESTAMP())::TIMESTAMP_NTZ) AS load_date,\n      TO_TIMESTAMP_NTZ('2099-12-31 23:59:59') AS end_date\n    FROM {bronze_clean} src\n    WHERE NOT EXISTS (\n      SELECT 1 FROM {silver_table} tgt\n      WHERE tgt.{key} = src.{key}\n        AND tgt.end_date = {open_end_date_literal}\n    );\n    \"\"\"\n    session.sql(insert_new_sql).collect()\n    print(f\"‚úÖ Inserted new/updated rows into {silver_table} (load_date = UTC now, end_date = 2099-12-31)\")\n\n    # -----------------------\n    # 3) DELETES: expire active silver rows missing from bronze_clean\n    #    Mark them deleted by setting end_date = current load timestamp (seconds precision, UTC)\n    # -----------------------\n    expire_deleted_sql = f\"\"\"\n    UPDATE {silver_table}\n    SET end_date = DATE_TRUNC('SECOND', CONVERT_TIMEZONE('UTC', CURRENT_TIMESTAMP())::TIMESTAMP_NTZ)\n    WHERE end_date = {open_end_date_literal}\n      AND {key} NOT IN (SELECT {key} FROM {bronze_clean});\n    \"\"\"\n    session.sql(expire_deleted_sql).collect()\n    print(f\"‚úÖ Expired rows in {silver_table} that are missing from today's bronze (logical deletes)\")\n\nprint(\"‚úÖ Step 3 complete: updates/insertions/deletes applied (SCD2 semantics).\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56325cf1-8052-4d40-8a38-e80e7a81fe8d",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "session.table(\"silver_dlt_orders\").order_by(\"order_id\").show(15)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1227fd4-70b2-4028-b88d-40135567b794",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# ---------- Step 4: Export updated silver tables back to S3 (parquet) ----------\n# Stage silver_csv should point to s3://datascience-output-bucket/silver/\nfor entity, meta in entities.items():\n    silver_table = meta['silver_table']\n    # Use entity capitalized name in file if you want (Customers/Products/Orders)\n    capital_entity = entity.capitalize()\n    copy_out_sql = f\"\"\"\n    COPY INTO @silver_csv/{entity}/Full_load/csv_files/{folder_path}{capital_entity}.parquet\n    FROM (SELECT * FROM {silver_table})\n    FILE_FORMAT = (TYPE = PARQUET COMPRESSION = SNAPPY)\n    SINGLE = TRUE\n    OVERWRITE = TRUE;\n    \"\"\"\n    session.sql(copy_out_sql).collect()\n    print(f\"‚úÖ Exported {silver_table} -> @silver_csv/{entity}/Full_load/csv_files/{folder_path}{capital_entity}.parquet\")\n\n# ---------- Finish ----------\nsession.close()\nprint(\"üîí Session closed. Full load complete.\")\n",
   "execution_count": null
  }
 ]
}